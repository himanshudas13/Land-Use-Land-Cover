{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCSZB2KFSsZWNxZlGuHraU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshudas13/Land-Use-Land-Cover/blob/master/Land-Use-Land-Cover%20/Prediction/lulc_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hjBsCrfNEWG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LSTM, TimeDistributed, Dense, Reshape, Input,Flatten, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Dense, Flatten, Reshape, Input\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved images\n",
        "\n",
        "input_shape = (448, 448, 3)  # Desired shape of input images\n",
        "time_steps = 2  # Number of time steps in the series\n",
        "data_dir = '/content/Classified'\n",
        "\n",
        "\n",
        "def load_and_preprocess_images(data_dir, input_shape):\n",
        "    images = []\n",
        "    for filename in sorted(os.listdir(data_dir)):\n",
        "        if filename.endswith('.tif') or filename.endswith('.png'):\n",
        "            img_path = os.path.join(data_dir, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            print(img.shape)\n",
        "            img = cv2.resize(img, (input_shape[0], input_shape[1]))\n",
        "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert from BGR to RGB for Matplotlib\n",
        "            plt.show()\n",
        "            img = img / 255.0  # Normalize to [0, 1]\n",
        "            print(f'{img.min()} n {img.max()}')\n",
        "\n",
        "            images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Load images\n",
        "images = load_and_preprocess_images(data_dir, input_shape)"
      ],
      "metadata": {
        "id": "XcQkXTL8NS3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split train and test set\n",
        "image_train=images[1:4]\n",
        "image_test=images[3:5]"
      ],
      "metadata": {
        "id": "zrT7Xq8JNgJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_lstm( encoded_features, time_steps):\n",
        "\n",
        "    # Prepare time series data\n",
        "    X, y = [], []\n",
        "    for i in range(len(encoded_features) - time_steps):\n",
        "        # Input sequence (time_steps images)\n",
        "        X.append(encoded_features[i:i + time_steps])\n",
        "\n",
        "        # Output (next feature vector or target)\n",
        "        y.append(encoded_features[i + time_steps])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "60fVLNHiOEE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = prepare_data_for_lstm(image_train, time_steps)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "siWMxqhPOFQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = prepare_data_for_lstm(image_test, time_steps)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "Q1DrBy8bOLcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_encoder(input_shape):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "\n",
        "    # Apply Conv2D and MaxPooling2D inside TimeDistributed\n",
        "    model.add(layers.TimeDistributed(layers.Conv2D(8, (3, 3), activation='relu', padding='same')))\n",
        "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
        "\n",
        "    # model.add(layers.TimeDistributed(layers.Conv2D(16, (3, 3), activation='relu', padding='same')))\n",
        "    # model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
        "\n",
        "    model.add(layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation='relu', padding='same')))\n",
        "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
        "\n",
        "    # Flatten the features\n",
        "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
        "\n",
        "    # Reduce dimensions to 16\n",
        "    model.add(layers.TimeDistributed(layers.Dense(32, activation='relu')))\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "P9SWu2C2OTf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = ( 2, 448, 448, 3)  # Sequence length of 2, image size 128x128, 3 channels\n",
        "cnn_encoder = create_cnn_encoder(input_shape)\n",
        "cnn_encoder.summary()"
      ],
      "metadata": {
        "id": "6maOjtelOUOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_encoded=cnn_encoder.predict(X_train)\n",
        "print(img_encoded.shape)"
      ],
      "metadata": {
        "id": "cxttS43zOjRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lstm_model(encoded_feature_shape, time_steps):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Input Layer\n",
        "    model.add(layers.Input(shape=(time_steps, *encoded_feature_shape)))\n",
        "\n",
        "    # RNN layer 1 with reduced units\n",
        "    model.add(layers.SimpleRNN(16, return_sequences=True, activation='relu'))\n",
        "\n",
        "    # RNN layer 2 with reduced units\n",
        "    model.add(layers.SimpleRNN(8, return_sequences=True, activation='relu'))\n",
        "\n",
        "    # RNN layer 3 with 16 units to output a feature vector\n",
        "    model.add(layers.SimpleRNN(32, return_sequences=False, activation='relu'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "iCTxAq3iOj9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_decoder_model(latent_dim, output_shape=(452, 452, 3)):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Input Layer\n",
        "    model.add(layers.InputLayer(input_shape=(latent_dim,)))\n",
        "\n",
        "    # Dense Layer to expand back to a higher-dimensional feature map\n",
        "    model.add(layers.Dense(56 * 56 * 64, activation='relu'))  # Adjust size to match an intermediate feature map size\n",
        "    model.add(layers.Reshape((56, 56, 64)))  # Reshape to (56, 56, 64)\n",
        "\n",
        "    # Upsampling and Conv2D Layers to progressively reconstruct the image\n",
        "    model.add(layers.UpSampling2D((2, 2)))  # (56, 56, 64) -> (112, 112, 64)\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    model.add(layers.UpSampling2D((2, 2)))  # (112, 112, 64) -> (224, 224, 64)\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    model.add(layers.UpSampling2D((2, 2)))  # (224, 224, 64) -> (448, 448, 64)\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    # Adjust to the target size\n",
        "    model.add(layers.Cropping2D(cropping=((0, 0), (0, 0))))  # Crop to (452, 452)\n",
        "    model.add(layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same'))  # Output shape (452, 452, 3)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "d32wC52sOmH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_model = create_decoder_model(decoder_input.shape[1], ( 452, 452, 3))\n",
        "decoder_model.summary()"
      ],
      "metadata": {
        "id": "WpDm7Jy4Osxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_image=decoder_model.predict(decoder_input)\n",
        "print(predicted_image.shape)"
      ],
      "metadata": {
        "id": "GaDD9Zr9O-FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the full model\n",
        "def create_full_model(cnn_encoder, lstm_model, decoder_model, time_steps):\n",
        "\n",
        "    cnn_input = layers.Input(shape=input_shape)\n",
        "\n",
        "    cnn_output = cnn_encoder(cnn_input)\n",
        "\n",
        "\n",
        "\n",
        "    # cnn_output_reshaped = layers.Reshape((time_steps, *cnn_encoder.output_shape[1:]))(cnn_output)\n",
        "    # # task 3: no need to reshape u can directly take cnn_encoder_output\n",
        "\n",
        "\n",
        "    # Pass through LSTM\n",
        "    lstm_output = lstm_model(cnn_output)\n",
        "\n",
        "    # Pass through Decoder\n",
        "    decoded_image = decoder_model(lstm_output)\n",
        "\n",
        "    # Define the full model\n",
        "    full_model = models.Model(inputs=cnn_input, outputs=decoded_image)\n",
        "\n",
        "    return full_model\n",
        "\n",
        "full_model = create_full_model(cnn_encoder, lstm_model, decoder_model, time_steps)\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "learning_rate = 0.001  # Adjust this value based on your needs\n",
        "\n",
        "# Create the Adam optimizer with the custom learning rate\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Compile and train the full model\n",
        "full_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "full_model.fit(X_train,y_train,epochs=20, batch_size=10, validation_split=0.0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nRAv8bxAPA-i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}